{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning the llama-2 7b chat model with LORA technique👇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello 👋\n",
    "\n",
    "In this Notebook I will walk you through the steps required to fine-tune a Llama-2 7b model on your own Dataset and will learn about all different tunings that can be done !!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets Start by importing few libraries :\n",
    "\n",
    "1) accelerate \n",
    "2) peft \n",
    "3) transformers \n",
    "4) bitsandbytes \n",
    "5) trl \n",
    "\n",
    "Add -q for quiet mode \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q accelerate peft bitsandbytes transformers trl dataset torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel , prepare_model_for_kbit_training , get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Developing a dataset and Exploring Chat Template / Prompt Template\n",
    "\n",
    "Every model has a chat template common one is the ChatML but in the model description you can find it ( Basically its the way the the dataset was transformed to train the model on the original dataset)\n",
    "\n",
    "Like here we are using the Meta's Llama 2 7b model (meta-llama/Llama-2-7b-chat-hf) this is a private model and needs a auth from the Meta \n",
    "\n",
    "So in this tutorial I am using another repo i.e. NousResearch/Llama-2-7b-chat-hf \n",
    "\n",
    "For chat template usually in the documentation of the hugging face repo you can find what template that model is using its important to do this before formatting your own dataset , here in our case the llama-2 7b model uses the following chat template :\n",
    "\n",
    "![Llama-2-template](Images/llama-2-m1.png)\n",
    "\n",
    "For more information look this link :https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L44\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No worries if you didnt went throught the above links !! I have done that for you 😉\n",
    "\n",
    "\\<s\\>[INST] \\<\\<SYS\\>\\> System Prompt \\<\\<\\/SYS\\>\\> User Prompt \\[\\/INST\\] Answer \\</s\\>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need your data to be in .jsonl format with input and output described as follows :\n",
    "\n",
    "![custom-dataset](Images/custom_dataset.png)\n",
    "\n",
    "(this is the dummy dataset , as I didnt want to expose the real dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_prompt(example):\n",
    "    bos_token = \"<s>[INST] <<SYS>>\"\n",
    "    user_inst = \"You are a custom AI model and your role is to output correct Code values for the Devices\"\n",
    "    eoi_token = \"<</SYS>>\"\n",
    "    before_prompt = '' # add your instruction before the actual input ( eg : this is the medical coding, tell me the ... , what is the ... )\n",
    "    input_prompt = example['Input'] + '[/INST]'\n",
    "    output_prompt = example['Output']\n",
    "    eos_token = \"</s>\"\n",
    "    \n",
    "    return bos_token + user_inst + eoi_token + before_prompt + input_prompt + output_prompt + eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset defined above ( this takes high RAM )\n",
    "train_ds = load_dataset('json' , data_files = 'Data/datafile' , field = 'train')\n",
    "test_ds = load_dataset('json' , data_files = 'Data/datafile' , field = 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the base model from 🤗\n",
    "\n",
    "\n",
    "BitsAndBytesConfig : https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'NousResearch/Llama-2-7b-chat-hf'\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, #if your gpu supports it \n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_use_double_quant = False #this quantises the quantised weights\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training_tokenizer (https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained)\n",
    "\n",
    "# https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    truncation_side = \"right\",\n",
    "    padding_side=\"right\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find appropriate length for your dataset for me 350-400 works for me !!\n",
    "\n",
    "This is your dataset length that you want to send into the model ... aka context window "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up lora for the quantisation \n",
    "\n",
    "Find the target modules you need to quantise in order to make the Lora finetuning work !\n",
    "The target modules can be found out by printing the model arch and knowing which layers you want to quantise  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.gradient_checkpointing_enable() #this to checkpoint grads \n",
    "model = prepare_model_for_kbit_training(base_model) #quantising the model (due to compute limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/TimDettmers/bitsandbytes/blob/main/bitsandbytes/nn/modules.py#L271\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printParameters(model):\n",
    "    trainable_param = 0\n",
    "    total_params = 0\n",
    "    for name , param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_param += param.numel()\n",
    "            \n",
    "            \n",
    "    print(f\"Total params : {total_params} , trainable_params : {trainable_param} , trainable % : {100 * trainable_param / total_params} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the target modules you want to apply Lora technique !! \\\n",
    "\\\n",
    "Here in our case we will be applying them on : \\\n",
    "\\\n",
    "![lora-paper](./Images/lora-1.png)\n",
    "\\\n",
    "\\\n",
    "q_proj , k_proj , v_proj , o_proj , gate_proj , up_proj , down_proj , lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LoraConfig : https://github.com/huggingface/peft/blob/main/src/peft/tuners/lora/config.py#L44\n",
    "\n",
    "# Its better to pass the values to the target_modules either \"all-linear\" or specific modules you need to quantise !!\n",
    "\n",
    "# You can change these parameters depending on your use case\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\",\n",
    "    target_modules=[ \n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\",\n",
    "    \"lm_head\",\n",
    "    ],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model , peft_config)\n",
    "printParameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L161\n",
    "\n",
    "# max_steps and num_train_epochs : \n",
    "# 1 epoch = [ training_examples / (no_of_gpu * batch_size_per_device) ] steps\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "  output_dir = \"LLama-2 7b\",\n",
    "  # num_train_epochs=1000,\n",
    "  max_steps = 1000, # comment out this line if you want to train in epochs\n",
    "  per_device_train_batch_size = 4,\n",
    "  warmup_steps = 0.03,\n",
    "  gradient_accumulation_steps = 1,\n",
    "  logging_steps=10,\n",
    "  logging_strategy= \"steps\",\n",
    "  save_strategy=\"steps\",\n",
    "  save_steps = 10,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  eval_steps=10, # comment out this line if you want to evaluate at the end of each epoch\n",
    "  learning_rate=2.5e-5,\n",
    "  bf16=True, #if your gpus supports this \n",
    "  logging_nan_inf_filter = False, #this helps to see if your loss values is coming out to be nan or inf and if that is the case then you may have ran into some problem \n",
    "  # lr_scheduler_type='constant',\n",
    "  save_safetensors = True,\n",
    ")    \n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=model,\n",
    "  peft_config=peft_config,\n",
    "  max_seq_length=max_seq_length,\n",
    "  tokenizer=tokenizer,\n",
    "  packing=True,\n",
    "  formatting_func=generate_dataset_prompt, # this will apply the generate_dataset_prompt to all training and test dataset mentioned above !!\n",
    "  args=args,\n",
    "  train_dataset=train_ds[\"train\"],\n",
    "  eval_dataset=test_ds[\"train\"]\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting output from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the trained model and generate some outputs from it \n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model , 'Checkpoint/base-checkpoint-10') #replace with the actual checkpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"<s>[INST] <<SYS>> You are a coding model and your goal is to correctly tell the medical codes to the user based on the prompt they have entered and you get rewarded for correct output <</SYS>> Tell me the medical code for cholera disease [/INST]\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=150, repetition_penalty=1.15)[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wooh !! The model just works fine and generates some cool outputs 😃\n",
    "\n",
    "I hope you enjoyed this tutorial on Llama-2 7b model and was able to create a custom LLM just for your use case !! If you have any doubts just create a issue in the repo or create a pull request for the same \n",
    "\n",
    "Also smash that star button to get more amazing tutorials from me !! 🐱🐱🐱🐱"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
