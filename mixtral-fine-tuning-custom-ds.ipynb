{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-1 Installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft bitsandbytes transformers trl dataset torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel , prepare_model_for_kbit_training , get_peft_model\n",
    "from trl import SFTTrainer\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-2 Getting the base model and tokenizer \n",
    "\n",
    "https://discuss.huggingface.co/t/understanding-how-changing-bnb-4bit-compute-dtype-affects-outputs/52167"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = 'mistralai/Mixtral-8x7B-v0.1'\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, #if your gpu supports it \n",
    "    bnb_4bit_quant_type = \"nf4\",\n",
    "    bnb_4bit_use_double_quant = False #this quantises the quantised weights\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
    "#make sure to give distribute appropriately , I will let this up to \"auto\" strategy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training_tokenizer (https://huggingface.co/docs/transformers/v4.37.2/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained)\n",
    "# https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    truncation_side = \"right\",\n",
    "    padding_side=\"right\",\n",
    "    add_eos_token=True,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-3 Quantizing the base model and loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = load_dataset(\"json\" , data_files = 'codes.jsonl' , field = \"train\")\n",
    "test_ds = load_dataset(\"json\" , data_files = 'codes.jsonl' , field = \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.gradient_checkpointing_enable() #this to checkpoint grads \n",
    "model = prepare_model_for_kbit_training(base_model) #quantising the model (due to compute limits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-4 Creating a prompt template\n",
    "\n",
    "Every model has a chat template (eg common one is the ChatML) in the model description you can find it \n",
    "\n",
    "Like here we are using the Mistral AI (mistralai/Mixtral-8x7B-v0.1) \n",
    "\n",
    "For chat template usually in the documentation of the hugging face repo you can find what template that model is using its important to do this before formatting your own dataset , here in our case the Mixtral-8x7B-v0.1 model uses no specific prompt template \n",
    "\n",
    "![mixtral-template](Images/mixtral-m1.png)\n",
    "\n",
    "\n",
    "So we will be using our base template that is :\n",
    "\\<s\\>[INST]  System Prompt \\n User Prompt \\[\\/INST\\] Answer \\</s\\>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPrompt(example):\n",
    "    bos_token = '<s>'\n",
    "    system_prompt = '[INST] You are a medical coding  model and your role is to give the medical codes \\n'\n",
    "    input_prompt = f\" {example['Input']} [/INST]\"\n",
    "    output_prompt = f\"{example['Output']} </s>\"\n",
    "    \n",
    "    return bos_token + system_prompt + input_prompt + output_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-5 Using the PEFT technique for finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printParameters(model):\n",
    "    trainable_param = 0\n",
    "    total_params = 0\n",
    "    for name , param in model.named_parameters():\n",
    "        total_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_param += param.numel()\n",
    "            \n",
    "            \n",
    "    print(f\"Total params : {total_params} , trainable_params : {trainable_param} , trainable % : {100 * trainable_param / total_params} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1, \n",
    "    bias=\"none\",\n",
    "    target_modules=[  #find the target modules that you want to \n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\",\n",
    "    \"lm_head\",\n",
    "    ],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-6 Creating a PEFT model and training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model , peft_config)\n",
    "printParameters(model)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/v4.37.2/src/transformers/training_args.py#L161\n",
    "\n",
    "# max_steps and num_train_epochs : \n",
    "# 1 epoch = [ training_examples / (no_of_gpu * batch_size_per_device) ] steps\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "  output_dir = \"LLama-2 7b\",\n",
    "  # num_train_epochs=1000,\n",
    "  max_steps = 1000, # comment out this line if you want to train in epochs\n",
    "  per_device_train_batch_size = 4,\n",
    "  warmup_steps = 0.03,\n",
    "  gradient_accumulation_steps = 1,\n",
    "  logging_steps=10,\n",
    "  logging_strategy= \"steps\",\n",
    "  save_strategy=\"steps\",\n",
    "  save_steps = 10,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  eval_steps=10, # comment out this line if you want to evaluate at the end of each epoch\n",
    "  learning_rate=2.5e-5,\n",
    "  bf16=True, #if your gpus supports this \n",
    "  logging_nan_inf_filter = False, #this helps to see if your loss values is coming out to be nan or inf and if that is the case then you may have ran into some problem \n",
    "  # lr_scheduler_type='constant',\n",
    "  save_safetensors = True,\n",
    ")    \n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=model,\n",
    "  peft_config=peft_config,\n",
    "  max_seq_length=350,\n",
    "  tokenizer=tokenizer,\n",
    "  packing=True,\n",
    "  formatting_func=createPrompt, # this will apply the generate_dataset_prompt to all training and test dataset mentioned above !!\n",
    "  args=args,\n",
    "  train_dataset=train_ds[\"train\"],\n",
    "  eval_dataset=test_ds[\"train\"]\n",
    ")   \n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-7 Generating outputs from the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the trained model and generate some outputs from it \n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model , 'Checkpoint/base-checkpoint-10') #replace with the actual checkpoint name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"<s>[INST] You are a coding model and your goal is to correctly tell the medical codes to the user based on the prompt they have entered and you get rewarded for correct output \\n Tell me the medical code for cholera disease [/INST]\"\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=150, repetition_penalty=1.15)[0], skip_special_tokens=True))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
